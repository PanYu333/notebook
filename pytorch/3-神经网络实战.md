# LeNet分类

## `LeNet`模型结构

![image-20230321221306623](D:\Programming code\markdown\notebook\pytorch\3-神经网络实战\image-20230321221306623.png)





---

## model.py

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class LeNet(nn.Module):
    
    def __init__(self):
        super(LeNet, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, 5)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(16, 32, 5)
        self.pool2 = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(32*5*5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)
    
    def forward(self, x):
        x = F.relu(self.conv1(x))       # input(3, 32, 32),  output(16, 28, 28)
        x = self.pool1(x)               # output(16, 14, 14)
        x = F.relu(self.conv2(x))       # output(32, 10, 10)
        x = self.pool2(x)               # output(32, 5, 5)
        x = x.view(-1,32 * 5 * 5)       # flatten
        x = F.relu(self.fc1(x))         # output(120)
        x = F.relu(self.fc2(x))         # output(84)
        x = self.fc3(x)                 # output(10)
        
        return x

```



**模型**: 2 * (卷积层 - 激活 - 池化层)  + 3  * 线性层

**`input shape`**: `(batch_size, 3, 32, 32)`

**卷积后宽度公式**:  (原宽度 - 核宽度 + 2 * pad) / stride + 1

- 在进入线性层的时候会将==(C, H, W)==打平成一维

<br>



---

## train.py



```python
import torch
import torchvision
import torch.nn as nn
import torch.utils.data
import torch.optim as optim
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np

from model import LeNet

# 数据预处理，Compose将预处理打包
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

# 下载训练集50000张训练图片
train_set = torchvision.datasets.CIFAR10(root='./data', train=True,
                                         download=False, transform=transform)
# 数据预处理
# train_loader
train_loader = torch.utils.data.DataLoader(train_set, batch_size=36,
                                           shuffle=True, num_workers=0)

# 下载测试集
test_set = torchvision.datasets.CIFAR10(root='./data', train=False,
                                        download=False, transform=transform)
test_loader = torch.utils.data.DataLoader(test_set, batch_size=10000,
                                          shuffle=True, num_workers=0)

# 构建迭代器
test_data_iter = iter(test_loader)
# 获取元素
test_image, test_label = next(test_data_iter)			# test_image(10000, 3, 32, 32), test_label(10000,)

# test_label中数字标签对应的含义
classes = ('plane', 'car', 'bird', 'cat', 'deer',
           'dog', 'frog', 'horse', 'ship', 'truck')

net = LeNet()
loss_function = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)

# loop over the dataset multiple times
for epoch in range(5):
    running_loss = 0.0
    for step, data in enumerate(train_loader, start=0):
      # step总数：50000/36=1388
        inputs, labels = data		#inputs(36, 3, 32, 32), labels(36,)

        optimizer.zero_grad()       # 梯度清零

        # forward + backward + optimize
        outputs = net(inputs)				# outputs(36, 10)
        loss = loss_function(outputs, labels)
        loss.backward()
        optimizer.step()

        #查看每500step的平均损失
        running_loss += loss.item()
        if step % 500 == 499:
            with torch.no_grad():       	# with 是一个上下文管理器， 不计算误差损失梯度
                outputs = net(test_image)	# outputs(10000, 10)
                predict_y = torch.max(outputs, dim=1)[1]	# predict_y(10000,)
                accuracy = torch.eq(predict_y, test_label).sum().item() / test_label.size(0)

                print(' [%d, %5d] train_loss: %.3f test_accuracy: %.3f' %
                      (epoch+1, step+1, running_loss / 500, accuracy))
                running_loss = 0

print('Finished Training')

# 保存模型参数
save_path = './Lenet.pth'
torch.save(net.state_dict(), save_path)
```

**数据集的划分规则：**

- **train_loader**:  batch_size = 36， 所以将50000个训练数据划分为step = 50000/36=1388组data，每组data里包含图片和对应的label，  即( imput(36, 3, 32, 32), label(36) )为一组

- **test_loader**:  batch_size = 10000, 即全部数据作为一个批次， 所以里面只有一组数据，`(inputs(10000, 3, 32, 32), label(10000))`

	<br>

**`iter()`函数**：

- `iter()`是Python的内置函数之一，它用于将可迭代对象转换为迭代器。在Python中，可迭代对象是指可以返回一个迭代器的对象，比如列表、元组、字典、字符串、集合等。

- `iter()`函数接受一个可迭代对象作为参数，并返回一个迭代器对象。这个迭代器对象可以用于迭代可迭代对象中的元素。

​		可以使用`next()`函数逐个获取元素，或者使用`for`循环遍历元素。

其中`test_image` 和 `test_label`是查看训练效果用的

<br>



---

## predict.py

```python
import torch
import torchvision.transforms as transforms
from PIL import Image
from model import LeNet

transform = transforms.Compose(
    [transforms.Resize((32, 32)),
     transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]
)

classes = ('plane', 'car', 'bird', 'cat', 'deer',
           'dog', 'frog', 'horse', 'ship', 'truck')

# 实例化模型并加载参数
net = LeNet()
net.load_state_dict(torch.load('Lenet.pth'))

im = Image.open('1.jpg')


im = transform(im)      # # 图片形状必须是(3, 32, 32), [C, H, W]
im = torch.unsqueeze(im, dim=0)     #扩展维度batch_size, [N, C, H, W]

with torch.no_grad():
    outputs = net(im)
    predict = torch.max(outputs, dim=1)[1].data.numpy()
		# predict = torch.softmax(outputs, dim=1).argmax(dim=1).numpy()
    
print(classes[int(predict)])
```

<br>

<br>

---

# AlexNet

## `AlexNet`模型结构

![image-20230322140419775](D:\Programming code\markdown\notebook\pytorch\3-神经网络实战\image-20230322140419775.png)

图中`MaxPooling`未作图。**这里卷积核的个数是两块GPU的总和，所以一块应该为**<font size=5>$\frac{kernel\_num}2$</font>

该网络的亮点在于： 

1. 首次利用 GPU 进行网络加速训练。 

2. 使用了 ReLU 激活函数，而不是传统的 Sigmoid 激活函数以及 Tanh 激活函数。 

3.  使用了 LRN 局部响应归一化。

4. 在全连接层的前两层中使用了 Dropout 随机失活神经元操作，以减少过拟合。

	<br>

	

**网络信息：**

![image-20230322140612970](D:\Programming code\markdown\notebook\pytorch\3-神经网络实战\image-20230322140612970.png)

<br>



---

## model.py

```python
import torch
import torch.nn as nn


class AlexNet(nn.Module):

    def __init__(self, num_classes=1000, init_weights=False):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            # 实际上是padding为(1,2),这里简化操作
            nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=2),  # input(3, 224, 224), output(48, 55, 55)
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),              # output(48, 27, 27)
            nn.Conv2d(48, 128, kernel_size=5, padding=2),       # output(128, 27, 27)
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),              # output(128, 13, 13)
            nn.Conv2d(128, 192, kernel_size=3, padding=1),      # output(192, 13, 13)
            nn.ReLU(inplace=True),
            nn.Conv2d(192, 192, kernel_size=3, padding=1),      # output(192, 13, 13)
            nn.ReLU(inplace=True),
            nn.Conv2d(192, 128, kernel_size=3, padding=1),      # output(128, 13, 13)
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2)               # output(128, 6, 6)
        )
        # 全连接层部分, 到此部分需要先展平数据
        self.classifier = nn.Sequential(
            nn.Dropout(p=0.5),          # 定义到下一层的dropout
            nn.Linear(128*6*6, 2048),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5),
            nn.Linear(2048, 2048),
            nn.ReLU(inplace=True),
            nn.Linear(2048, num_classes)
        )

        if init_weights:        # 实际上默认初始化为凯明方法值
            self._initialize_weights()

    def forward(self, x):
        x = self.features(x)
        x = torch.flatten(x, start_dim=1)       # 从维度为1后开始打平，0维是batch_size
        x = self.classifier(x)

        return x

    def _initialize_weights(self):
        """
        对权重初始化, 可选，torch有默认的初始化参数
        """
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)    # 正常初始化为0
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)  # 正太分布，均值0，方差0.01
                nn.init.constant_(m.bias, 0)
```

<br>



---

## train.py

```python
import os
import sys
import json
import torch.nn as nn
import torch.utils.data
import torch.optim as optim
from torchvision import utils, transforms, datasets
import tqdm

from model import AlexNet

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("using {} device".format(device))

data_transform = {
    "train": transforms.Compose([transforms.RandomResizedCrop(224),  # 随机裁剪至(224， 224)
                                 transforms.RandomHorizontalFlip(),  # 水平翻转，默认概率0.5
                                 transforms.ToTensor(),
                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),
    # 在预测过程中，不必要对图片裁剪，那样失去了预测的意义，压缩即可
    "val": transforms.Compose([transforms.Resize((224, 224)),  	 # 是一个压缩操作,can not be 224, must be (224,224)
                               transforms.ToTensor(),
                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
}

data_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))  # get data set root path
image_path = data_root + "/data_set/flower_data/"  							 # flower data set path
assert os.path.exists(image_path), "{} path does not exist.".format(image_path)
# 加载图片训练数据集3306张并预处理
train_dataset = datasets.ImageFolder(root=image_path + "/train",
                                     transform=data_transform["train"])

train_num = len(train_dataset)
# {'daisy':0, 'dandelion':1, 'roses':2, 'sunflower':3, 'tulips':4}
flower_list = train_dataset.class_to_idx
class_dict = dict((value, key) for key, value in flower_list.items())  # 换个方向

# write dict into json file
json_str = json.dumps(class_dict, indent=4)  # indent(缩进)
with open('class_indices.json', 'w') as json_file:
    json_file.write(json_str)

# 训练批次
batch_size = 32

# 训练集训练前处理
train_loader = torch.utils.data.DataLoader(train_dataset,
                                           batch_size=batch_size, shuffle=True,
                                           num_workers=0)

# 验证集预处理
validate_dateset = datasets.ImageFolder(root=image_path + "/val",
                                        transform=data_transform["val"])
val_num = len(validate_dateset)
validate_loader = torch.utils.data.DataLoader(validate_dateset,
                                              batch_size=4, shuffle=False,
                                              num_workers=0)

print("using {} images for training, {} images for validation.".format(train_num, val_num))

# # 将validate_loader中的batch_size=4, shuffle=True,查看数据和标签
# test_data_iter = iter(validate_loader)
# test_image, test_label = next(test_data_iter)
#
#
# def imshow(img):
#     img = img / 2 + 0.5  # unnormalize
#     npimg = img.numpy()
#     plt.imshow(np.transpose(npimg, (1, 2, 0)))
#     plt.show()
#
#
# print(' '.join('%5s' % class_dict[test_label[j].item()] for j in range(4)))
# imshow(utils.make_grid(test_image))

# 开始训练
net = AlexNet(num_classes=5, init_weights=True)
net.to(device)
loss_function = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.0002)

epochs = 10
save_path = './AlexNet.pth'
best_acc = 0.0
train_steps = len(train_loader)     # 3306 / 32 = 103 + 1

for epoch in range(epochs):
    # train
    net.train()     # 会启用dropout，BN等训练中才可以用的方法
    running_loss = 0.0

    train_bar = tqdm.tqdm(train_loader, file=sys.stdout)
    for step, data in enumerate(train_bar):
        images, labels = data   # images(32, 3, 224, 224), labels(32,)
        optimizer.zero_grad()
        outputs = net(images.to(device))    # outpus(32, 5)
        loss = loss_function(outputs, labels.to(device))
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        train_bar.desc = "train epoch[{}/{}] loss:{:.3f}".format(epoch + 1,
                                                                 epochs,
                                                                 loss)

    # 训练完一个epoch做一次验证
    net.eval()
    acc = 0.0
    with torch.no_grad():
        val_bar = tqdm.tqdm(validate_loader, file=sys.stdout)
        # step = 364 / 4 = 91
        for val_data in val_bar:
            val_images, val_labels = val_data   # val_images(4, 3, 224, 224), val_labels(4,)
            outputs = net(val_images.to(device))    # outputs(4, 5)
            predict_y = torch.max(outputs, dim=1)[1]
            acc += torch.eq(predict_y, val_labels.to(device)).sum().item()

    val_accurate = acc / val_num
    print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' %
          (epoch + 1, running_loss / train_steps, val_accurate))

    # 提高了精度就保存模型参数
    if val_accurate > best_acc:
        best_acc = val_accurate
        torch.save(net.state_dict(), save_path)

print('Finished Training')
```

<br>



---

## predict.py

```python
import os
import json

import torch
from PIL import Image
from torchvision import transforms
import matplotlib.pyplot as plt

from model import AlexNet


def main():
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    data_transform = transforms.Compose([transforms.Resize((224, 224)),
                                         transforms.ToTensor(),
                                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

    image_path = "./1.jpg"
    assert os.path.exists(image_path), "file: '{}' dose not exist.".format(image_path)
    img = Image.open(image_path)

    plt.imshow(img)
    # [N, C, H, W]
    img = data_transform(img)
    img = torch.unsqueeze(img, dim=0)

    json_path = "./class_indices.json"
    assert os.path.exists(json_path), "file: '{}' dose not exist.".format(json_path)
    with open(json_path, "r") as f:
        class_indict = json.load(f)

    model = AlexNet(num_classes=5).to(device)

    weights_path = "./AlexNet.pth"
    assert os.path.exists(weights_path), "file: '{}' dose not exist.".format(weights_path)
    model.load_state_dict(torch.load(weights_path))

    model.eval()
    with torch.no_grad():
        output = torch.squeeze(model(img.to(device))).cpu()
        predict = torch.softmax(output, dim=0)
        predict_cla = torch.argmax(predict).numpy()

    print_res = "class: {} prob{:.3}".format(class_indict[str(predict_cla)],
                                             predict[predict_cla].numpy())
    plt.title(print_res)
    for i in range(len(predict)):
        print("class: {:10}   prob: {:.3}".format(class_indict[str(i)],
                                                  predict[i].numpy()))
    plt.show()


if __name__ == '__main__':
    main()
```
